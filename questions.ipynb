{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sanskrirtisingh/opt/anaconda3/envs/data_project/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from depends.transform import generate_pdf_hi_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "elements = generate_pdf_hi_res(filename=\"data/qwen2audio.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from depends.question_gen import generate_questions, generate_questions_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = generate_questions(elements=elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What is the title of the paper that introduces the concept of Audio Flamingo?', 'Who are the authors of the paper \"Audio flamingo: A novel audio language model with few-shot learning and dialogue abilities\"?', 'What is the primary focus of the Audio Flamingo model?', 'What is the name of the multi-modal language model introduced in the paper \"Macaw-llm: Multi-modal language modeling with image, audio, video, and text integration\"?', 'Who are the authors of this paper?', 'What types of data do you think the Macaw-LLM model integrates for its training?', 'What is the name of the technical report published by OpenAI, which discusses the capabilities of their GPT-4 model?', 'In what year was this technical report released?', 'What is the name of the ASR corpus used in research, as mentioned in the document?', 'Who are the authors of the paper that introduces the BLEU metric for evaluating machine translation systems?', 'In what year was this paper published?', 'What is the name of the multimodal multi-party dataset introduced in the paper \"MELD: A multimodal multi-party dataset for emotion recognition in conversations\"?', 'Who are the authors of the paper that discusses SALMONN, a model designed to provide generic hearing abilities for large language models?', 'What is the name of the speech-to-text translation corpus introduced in the document?', 'Who are the authors of the papers \"Chen Wang, et al., Blsp: Bootstrapping language-speech pre-training via behavior alignment of continuation writing\" and \"Mingqiu Wang, et al., SLM: Bridge the thin gap between speech and text foundation models\"?', 'What is the name of the model discussed in the document that combines speech-to-text and large language model integration?']\n"
     ]
    }
   ],
   "source": [
    "print(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "print(len(questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = generate_questions_2(elements=elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What is the title of the first paper mentioned in the document?', 'Who are the authors of the paper \"SLM: Bridge the thin gap between speech and text foundation models\"?', 'What is the reference number of the paper \"SLM: Bridge the thin gap between speech and text foundation models\"?', 'Where was the paper \"SLM: Bridge the thin gap between speech and text foundation models\" published?', 'What is the position of the point (185.6715545654297, 620.6455555555556) on the page?', 'Who are the authors of the paper \"On decoder-only architecture for speech-to-text and large language model integration\"?', 'What is the reference number of the paper \"On decoder-only architecture for speech-to-text and large language model integration\"?', 'Where was the paper \"On decoder-only architecture for speech-to-text and large language model integration\" published?', 'What is the position of the point (186.8966064453125, 775.6205555555555) on the page?', 'Who are the authors of the paper \"Next-gpt: Any-to-any multimodal LLM\"?', 'What is the reference number of the paper \"Next-gpt: Any-to-any multimodal LLM\"?', 'Where was the paper \"Next-gpt: Any-to-any multimodal LLM\" published?', 'What is the position of the point (180.7640380859375, 1229.4733333333331) on the page?', 'Who are the authors of the paper \"Mmspeech: Multi-modal multi-task encoder-decoder pre-training for speech recognition\"?', 'What is the reference number of the paper \"Mmspeech: Multi-modal multi-task encoder-decoder pre-training for speech recognition\"?', 'Where was the paper \"Mmspeech: Multi-modal multi-task encoder-decoder pre-training for speech recognition\" published?']\n"
     ]
    }
   ],
   "source": [
    "print(questions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
